# TODO
SEARCH FOR (VOLKAN) / (MO) FOR TASKS

## Organizational
- [X] Modularize meeting Questions
- [ ] Write info in a separate document as a draft for report at the end 
- [X] figure out .gitignore for .idea/
  - [X] does it work now?? (MO)

---

## Environment
- [X] make env variables dictionary
### Obstacles -> is 2nd prio after SAC-X for midterm
- [ ] Obstacles:
  - [X] Added obstacle in env, where ever the “agent" is
      - [X] several obstacles (VOLKAN)
    - [ ] Collision detector in the env reward func
      - [X] first for static env
      - [ ] Dot product of agent_traj_grid * environment_traj_grid for 1 step
        - [ ] Possible improvement: Eg. if 1 step is 10 pixel movement only occupy pixel 6-10 for all agents as “prediction”
  - [X] Add the obstacle info NN	
  - [ ] Continuous action space 
    - [ ] only look at distance and size

### Collision Check
**2nd prio after SAC-X for midterm** 
- [ ] Continuous with trajectory prediction one step ahead

  - with the trajectory prediction one step ahead
  - if says might collide (e.g. following a car)
    - we can say we play safe and think all these situations as collision
    - if the obstacle moves away, then collision chance drops
    - if the obstacle perpendicular, then collision chance drops
    - control the velocity of the agent to avoid collision
    - more detailed version would include physic simulators with solver, there are papers for this
  - make the velocity range small (e.g. 20km/h)
  - have a delta time scaling to real velocity 
  - 
### Reward Func (VOLKAN) -> is 2nd prio after SAC-X for midterm
- sparse rewards only when there is a collision, or reach the goal
  - [X] 1 for goal, -1 for collision (from extensive testing in a project last semester) nothing more
    - [X] sub-sparse rewards distance: rewards like radius checkpoints when moving towards the goal is also possible
      - will make waiting behavior more likely, which wouldn't happen in dense rewards
      - but also don't reward it again for stepping over it again
      - can be a nice sub study
      - [X] make it an option in run
    - [ ] time: punish it every e.g. 100 time step to make it go towards to goal at one point (VOLKAN)
          or terminate the episode after 100 time steps and give a reward of -1 
      - [ ] implement action history for it
    - Examples of sub-sparse rewards: (added to the global reward) (VOLKAN) 
      - [ ] waiting behavior: reward for waiting for 10 time steps
      - [ ] moving at least 10 m in 1 direction 

---

## Algorithm
#### SAC
- [X] SAC QS
  - [X] why 2 critic networks
    - Double Q-Learning also in the lectures 
    - [X] see answer in piazza
      - TD3 learns two Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss        
        functions. https://spinningup.openai.com/en/latest/algorithms/td3.html 
    - if not it doesn't converge well otherwise
  - The other trick in SAC
    - [X] understand the following
    - in sac we take the entropy and it depends on random variables
    - we need the gradient of the entropy but we cant calculate it so we do reparametrization (MC Integral)
    - the noise is additive so we say its constant and just add it to the gradient
  - [X] with sparse rewards SAC-X doesnt work well and seems like it moves randomly
    - but that is exactly we need in a dynamic environment
    - 
##### JUMP START IDEA (MO)
  - [X] JUMP START SUB TEST
    - see line 4 in [Spinning Up SAC Pseudo Code](https://spinningup.openai.com/en/latest/algorithms/sac.html?highlight=SAC#pseudocode) 
    - [X] switch the action policy with A* motion planning results for first e.g. 10 replay buffer entries
          then continues with normal SAC
  - for the static scene there is a potential
        - SAC is a middle ground offline RL / on-policy RL depending the amount of action in the replay buffer 
        - fill the replay buffer with the pretraining on static env 
        - so that we already know 1 good policy that reaches the goal (but then we dont have a model free approach)
          - but we can also erase the replay buffer with these A* results after some episodes
      - **for dynamic scene it wouldn't work**
      - we can just assume that the agent found the perfect trajectory in history and add that to the replay buffer
        - this would still be a model free approach
        - the critic would learn the value of the trajectory in time 
        - BUT make it noisy 
        - but in real life we don't have this perfect trajectory for RL tasks
          - trade off between model free and model based
          - but this supervised approach might collapse for complex dynamic envs.
          - e.g. in a dynamic env with a lot of obstacles and a lot of possible trajectories, 
            we can't know the solution
### Network
- [X] make a hyper param dictionary
- [X] make NN initialization a function
- 
#### Inputs
- input-outputs of networks
  - actor network
    - in: position of the agent, position of the goal, position of the obstacles
    - out: mean and std of the velocity in 2 directions
  - critic network 1
    - in: action of agent and position of the agent, position of the goal, position of the obstacles
    - out: q value
  - critic network 2
  - value network
    - in: position of the agent, position of the goal, position of the obstacles
    - out: v value
  - sort the input depending on the distance to the robot 
    so that the network has a structured input (like attention with transformers)
  - 
#### Outputs
### Optimizer

---

## Performance
- Pytorch for parallel simulation(might not be needed), but keep an eye on performance
### Google Cloud
- [ ] (OPTIONAL) do the sparse to dense interpolation experiment with 10 parallel runs with slightly differing reward funcs
- use cpu and the cheaper machines (that google can shutdown)
- we can use different machines for different configurations 
- (e.g. 1 machine for each test from dense to sparse to find a sweet spot)
- use tensorboard or w&b for logging

---

## MILE STONES
### MIDTERM
- For the midterm presentation a possible milestones
- E.g. Comparing 500.000 env steps with our env, how well the dense/sparse rewards performed
    - 1st in static env
    - then in dynamic envs with random init for movements 
      - make sure to have a random seed to reproduce the results)



