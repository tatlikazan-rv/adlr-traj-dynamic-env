# 22 11 21 Weekly Meeting

## Organizational

## Environment
### Obstacles

### Collision Check
### Reward Func -> **after midterm prioritize SAC-X**

- a well-designed dense reward can speed up the training
- checkpoint wise sub sparse works
- [ ] thresholds for waiting and consistency 
- [ ] sub action for later on with SAC-X and Meta RL 
- [ ] SUB STUDY: 
  - [ ] sort the obstacles according the distance and give weights appropriately
    - [ ] take the time to collisions in to account calc depending on our speed  
  - [ ] or take the closest obstacle only
  - 
#### Sub sparse Rewards -> **after midterm prioritize SAC-X**

- [ ] target distance checkpoint reward _value_
  - doesn't really matter as we only give it several times and it cant be exploited
- [ ] waiting reward _value_
  - can be exploited, so we might have to punish it if it wait too much
- [ ] e.g. moving 10 step in 1 direction reward value
  
## Algorithm

#### SAC-X
- [ ] read paper again
- [X] extend SAC accordingly in a new folder

#### SAC
- [X] SAC-X better for motion planning against normal SAC?
  - yes for dynamic environments 

##### JUMP START IDEA (MO)
- [X] try giving every solution with a star in grid and see what happens
  - doesnt make sense we should give it at the start of every new environment 
~~- [ ] does it actually help, can we test it like the following: -> NOT NEEDED SEE BELOW~~
      ~~- [ ] gridsize: 20 -> 400 possible positions -> 160.000 possible paths~~
  ~~- pretrain:SAC_Train episode ratio~~ 
        ~~- 0:200 -> sigma ~ 1~~
        ~~- 50:200 -> sigma ~ 0.6~~
        ~~- 200:200 ->~~ 
        ~~- 200:10 ->~~  
        ~~- 200:1 ->~~ 
        ~~- num episodes not enough regarding the possible paths~~
    ~~- [ ] gridsize: 5 -> 25 possible positions -> 625 possible paths~~
        ~~- we can try if the astar solution for every target&start pair helps overall or not~~
        ~~- pretrain:SAC Train episode ratio~~
- [X] compute a new astar result for each new environment and add to the replay buffer
- [X] first with static env and with the "fill the replay buffer with optimal solution" trick
- [X] redefine the reward function for it
- this is better than a normal dense reward, dense reward gets complex  
- with astar path than sparse reward we can train quite faster
- [X] considers astar path as several states and give the rewards accordingly
- [X] technically give the all correct transitions needed in to the replay buffer with the according reward

### Policy
### Networks
#### Inputs
- [X] set NN seed for comparable result of features, then optimize later on the best model with random seeds
  - [X] choose a global seed
    - see model.py torch.manual_seed(3407)
  - sometimes the NN implementation doesn't allow it, but the rest of the algo should be with a seed for experiments to be deterministic

#### Outputs

## Optimizer
- [X] Reparam noise?
    - gives the actor net the ability to explore
    - see the code comments and 22-11-14-week for explanations
- [X]  Value loss 0.5?
    - might be a trick
    - check stable baselines or the original SAC paper
    - just an average of both critic nets
    - [X] check SAC Code

 
### Loss
- [X] negative loss?
  - comes from entropy  
- [X] plot the sigma for (VOLKAN)
  - we took the average sigma values of the states in a batch
- [X] weights and biases, should be free for student, alternative tensorboard (VOLKAN)

## Performance

### Curves
- [X] implement sigma plot
- [X] add wandb
  - https://wandb.ai/tum-adlr-09
<<<<<<< HEAD
  - [X] still needs some configurations
=======
  - [ ] still needs some configurations

>>>>>>> test_branch
### Google Cloud
- [X] Visual Introduction
- don't go over 15$ an hour
- performance use N2 , configure the machine type 
- d2d performance estimate
- for GPU you need to have a big enough batch! so that dataloading doesn't become bottleneck
  - nvidia T1 1 use 1 gpu
-boot disk
  - OS: ubuntu -> for specific software that only runs on ubuntu
  - disk size 64gb for installing drivers recommended
- Firewall e.g. for jupyter notebook
  - don't matter if we don't use the machines besides computing
- Standard setup:
  - google doesnt break 
  - reduce availibity policies
  - VM provisining model: spot!
  - use training checkpoints!!!
  - so that even if the machine is killed by google, there will be an email, we can keep going on from checkpoint to train
- for 2nd 500$ that is possible, but rarely happens, if we use spot machines we wont get to spend the first 500$ anyway
- VM
  - google cloud command line interface
  - google cloud command to terminal -> specify europe west
  - remote machine shows in commandline 
  - possible take data from Google Drive
  - but we can use the git - clone our repo in google 
  - we are connected via ssh, so it will stop in some time
  - use "screen" or "tmux"(more capable), it creates like a virtual command line in our command line
    - use one 1 virtual machine use one screen so that you didn't accidentally close it
  - most of the time nVidia is installed but check google help for needed command line calls
  - OR we can create a machine that copies our machine by a "snapshot" is full copy of the setup 
    - from 1 google cloud VM copy it again for new VMs so that it easier
  - WRITE FELIX FOR QUESTIONS!!!

## MILE STONES
- [X] how long should the milestone report be?
  - 3 pages with figures and references -> on the project page 
  - 
### MIDTERM




