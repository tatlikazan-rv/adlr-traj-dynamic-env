# 22 11 21 Weekly Meeting Questions

## Organizational

---

## Environment
- [X] set seed for environment e.g. environment: obs,    
  - how to have several environments with 1 seed?
    - [X] where should we define it (FELIX)
    - use the "vector env" so that we can create diff envs
    - [X] or use a list of seeds for env creation, by generating 1 seed and adding (VOLKAN)
### Obstacles
- [X] Felix if we should focus on SAC-X or moving obstacles for midterm (FELIX)
  - we prefer to switch the SAC-X first because SAC doesn't perform well even in static environment -> doesnt matter both should work
  - we should do SAC-X in grid static env first
  - [X] how to train SAC w/o jump start trick better for static environments (FELIX)
    - [X] sort obstacles depending on distance when give it in the network (VOLKAN)
    - [ ] hindsight replay buffer -> works well for super sparse reward (1 target, -1 crash) (after sorting obstacle and smooting)
      - it relabels trajectories in replay buffer that could be the correct trajectory for an another environment!
      - the agent can act "idle" basically avoid obstacle, and not reach to goal as  
      
### Collision Check
### Reward Func
- for super sparse reward SAC takes a long time to learn
#### Sub sparse Rewards

---

## Algorithm
#### SAC-X
- works on continuous envs
- SAC-X is like an improved hindsight replay buffer
#### SAC
- [X] SAC-X better for motion planning in static env against normal SAC?
  - should also work with grid static env with both, doesnt have to be continious
##### JUMP START IDEA - WARMSTART
- [ ] use this at midterm
  - normal SAC doesn't perform well
    - but it does train faster w. pretrain 
  - it converges to a large sigma
    - [X] on how to train the network for it (FELIX)
      - [X] try a lower entropy value (MO)
      - [ ] play with the hyperparameter alpha that decays over time (MO)
        - see spinning up implementation of alpha decay OR in original SAC for a dynamic adjusting alpha
  - [ ] try SAC-X
    - [X] try sorting first for several obstacles (VOLKAN)
    
### Policy
### Networks
#### Inputs
- [X] set NN seed for comparable result of features, then optimize later on the best model with random seeds
#### Outputs
- [X] filter the output by a moving average filter for better performance with SAC (MO)
- take a window of action history and take the average of the window (to reduce the high frequency actions) e.g. last 4
  - network output with high std -> filter the resulting actions -> then give it to the agent   
  - should reduce the amount of noise
    - a not smooth signal can lead to jittering
  - don't smooth it too much
    - bigger sliding window means smoother the curve for sigma
- [ ] plot both actions out of network and smoothed version to see the difference (VOLKAN)
## Optimizer
### Loss

---

## Performance
### Curves
- https://wandb.ai/tum-adlr-09
### Google Cloud
- [X] directly connected to wandb?
  - yes should directly work if it works in our code as well
## MILE STONES
- [ ] do a short google search for research based version for jump start idea -> wait for paper from FELIX  about it
  - (OPTIONAL) Felix found papers for SAC with prefilled buffers with different methods besides A-star 
    - e.g. Tesla stores driving data and they added a regularization to SAC to match that traj which didn't work
    - do mention it in the MILESTONE REPORT
    - giving prestraining
    - there is ongoing research going on about this
    - "Accelerating Online Reinforcement Learning with Offline Datasets" further reading on this idea
    - in our version with Astar we know the reward and fill the replay buffer with perfect trajectory
    - in Motion capture for Robots we dont know the reward
- [ ] SAC-X vs SAC in grid world 
### MIDTERM




