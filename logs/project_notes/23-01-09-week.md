

## Organizational

## Environment

### Obstacles

### Collision Check


### Reward Func
#### Sub sparse Rewards
  
## Algorithm
#### SAC-X
- [ ] start without pretraining to test the papers method
- [ ] train with scheduler
- [ ] run the robot on main task only which is the super sparse reward
  - [ ] possible problem: main task can get stuck
  - [ ] activate the needed subtask by check if the agent is not moving double the waiting  
- [ ] maybe an LSTM instead of MLP and leave MetaRL

- [ ] regular sac w/o pretrain STATIC
- [ ] regular sac-x w/o pretrain STATIC

- [ ] regular sac w/o pretrain DYNAMIC
- [ ] regular sac-x w/o pretrain DYNAMIC


- [ ] then sac with pretrain DYNAMIC
- [ ] then sac-x with pretrain DYNAMIC
  - OPTION 1
  - [ ] simulate that we used scheduler
  - OPTION 2
  - [ ] do pretrain on static initial 
  then let it figure out

- [ ] more subtasks than 3
  - [ ] waiting 
  - ...

#### SAC
- [ ] easy implementation of SAC to switch to SAC-X
- there is also SAC2 but we want the comparison with SAC and SAC-X so we dont use it
##### JUMP START IDEA

### Policy
### Networks
#### Inputs

#### Outputs

## Optimizer -> **after midterm**
### Loss
## Performance -> **after midterm**


### Curves -> **after midterm**


### Google Cloud -> **after midterm**


## MILE STONES



### MIDTERM PRESENTATION

### MIDTERM REPORT

### ENDTERM PRESENTATION
- [ ] make use of museum example 

#### as Appexdix
- [ ] model specifications
- [ ] maybe if we generate data we can talk about the algo
- [ ] pytorch, wandb, gcloud, own implementation of environment from open ai gym template

### ENDTERM REPORT
- [ ] cite all the algorithm names
- [ ] put numbers on the visual to indacte sorting
- [ ] checkpoint rewards: use more visuals 
  - put and arrow and +1 reward etc. 
  - 2nd arrow inside the area, no reward

- [ ] put standard deviation of the result table


- [ ] send a draft first, before the final version 

