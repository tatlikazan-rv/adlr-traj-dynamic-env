
## Organizational

## Environment
- [X] recompute a sampling rate for the animation for real time perception

### Continous
- [X] give agent velocity as well
- [X] set a delta_T dep on real time like .5s
  - [X] just multiply the velocity with it
  - we assume env.step() is e.g. 1000 hz, then show only every 1000th step in animation
  - [X] normalize the network velocity output between 1 & -1 -> sample_normal in ActorNet
- [X] 1 million training steps:
  - [X] action smoothing, some faster obstacles, velocity as output

- [X] astar cells should have at least the half the size of the smallest object
  - [X] if a cell is tengential to a obstacle its marked as occluded as well 
### Obstacles

### Collision Check
- [ ] only give the closest 2 obstacles during dynamic
  - like a field of view
  - octadrant as team 4 as a clooab test?
  - or does this go too much into the representation?
  - more obstacles in states mean possibility to develop strategy
  - explanation would be in an AD scenario this is the case, closest matter more
  - the global nagivation is solved by pretraining
    - pretrain on the initial state!! resembles more to the real scenario e.g. AD in roundabout with pedestrian crossing 
    - https://deepai.org/publication/pretraining-in-deep-reinforcement-learning-a-survey  
  - a nice exprement would be to train an agent w/o pretrain and compare, this would be completely un biased -> see SAC-X below

- [X] 1. check for all
  - [ ] test for different amount of predictino length 
if enough_time:
- [ ] 2. check foir closest 2
- [ ] 3. subtask: get away from obstacle compare obs_distance_diff and give reward if gets bigger
- [ ] 4. physics sim solver?? later

keep the distance(m) should be half the speed(kmh) to the closest obstacle and do that as a rewad for the agent

### Reward Func
#### Sub sparse Rewards
  
## Algorithm
#### SAC-X
- Scheduler:
  - [ ] last 3 subtasks and nearest obstacle + target state
  - worth trying, better than uniform scheduing anyway
  - [ ] ADD IT in to the report that we differ from SAC-X Scheduler!!
#### SAC


##### JUMP START IDEA
- transform the veloctiy output to location for astar in pretraining

### Policy
### Networks
#### Inputs
- [X] use the last state to calculate the velocity
- [X] give in the velocity of the agent in the state as well
  - [ ] possibly the acceleration as well if there is time to test

#### Outputs
- [X] only output the velocity

## Optimizer -> **after midterm**
### Loss
## Performance -> **after midterm**
- Model: upto 3 layers should be okey
  - but experiment with this at the end with the random search with other hyper parameters

### Curves -> **after midterm**
- [X] total reward: show which skill is learned or is it really learned

### Google Cloud -> **after midterm**


## MILE STONES



### MIDTERM PRESENTATION

### MIDTERM REPORT

### ENDTERM PRESENTATION

#### as Appexdix

### ENDTERM REPORT
- if Sac-x comparison doesnt work
  - we can still compare static/dynamic and with or without pretrining

### EXAM


## COLLAB TEAM 7 ?


500 Hz env update freq delta_t 1/500s, but controller has lower update freq for smoother behavior (meaning for examle that we run the training every second time the environment)
control max steps by time not nubmer of steps
increase the number of steps for action smoothing