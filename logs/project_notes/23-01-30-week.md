
## Organizational

## Environment
- vary obstacle size to have more interesting environments
### Continous
### Obstacles

### Collision Check

- [X] 1. check for all
  - [ ] test for different amount of prediction length 
if enough_time:
- [ ] 2. check foir closest 2
- [ ] 3. subtask: get away from obstacle compare obs_distance_diff and give reward if gets bigger
- [ ] 4. physics sim solver?? later

keep the distance(m) should be half the speed(kmh) to the closest obstacle and do that as a rewad for the agent

### Reward Func
- punish that the agent accelarates while its close to a obstacle?
  no because we might want to escape a situation
#### Sub sparse Rewards
  
## Algorithm
#### SAC-X
- dont use sac-u
- [ ] 6 Skills:
  - seeking obstacle (to follow a moving obstacle)
  - avoiding obstacle
  - seek target
  - flee from target (if we have to find a new way)
  - consistency (SR CONSISTENCY)
  - waiting (SR WAITING)
- obstacle: can be dense if its hard to implement 
  - seeking distance threshold: smaller aggressive threshold
  - avoiding distance threshold: bigger conservative threshold
- target 
  - seeking (SR CHECKPOINTS)
  - avoiding

#### ContSAC-X

- dont use sac-u

MAIN TASK:
- super-sparse rewards (-50, +10)
- Time penalty

- [X] 6 Skills: (Sparsa Rewards:SR)

  - Skill 1: seeking obstacle (to follow a moving obstacle)
    - dense threshold distance to obstacle
    - stay between 2-5 * radius of the obstacle

  - Skill 2: avoiding obstacle 
    - dense threshold distance to obstacle for all 
    - NO dynamic obstacles or for all if the latter proves to be hard
    - NO sparse rewards checkpoint to obstacle for static
  
  - Skill 3: seek target 
    - sparse rewards checkpoint to target
    - NOT distance to target -> SAC-X performs badly
  
  - NOPE Skill 4: flee from target (if we have to find a new way)
    - dense rewards distance to target
    - we didnt test this first as our agent is not capable enough yet 
    - [X] should be happening implicitly by using the waiting as it has also a penalty for waiting too long
 
  - Skill 5: consistency 
    - [X] consistency SR 
 
  - Skill 6: waiting 
    - [X] waiting SR 

- obstacle: can be dense if its hard to implement 
  - seeking distance threshold: smaller aggressive threshold
  - avoiding distance threshold: bigger conservative threshold
- target 
  - seeking (SR CHECKPOINTS)
  - avoiding
#### SAC


##### JUMP START IDEA

### Policy
### Networks
#### Input

#### Outputs

## Optimizer 
### Loss
## Performance 
- [X] test time by 10000 steps and see the bottle neck  
  - we can use numba compiler, and run the compiled code to have a faster run 
  - if we do tensor calcs (parallelized env calculations) e.g. in PPO, but its different in SAC as its sequences
  - max episode length
  - OVERDUE: as highcpu in google is fast enough that this is low priority now  

### Curves 
- [ ] check the mean episode reward
  - start start with 0 and go towards the value of target reached 

### Google Cloud 


## MILE STONES



### MIDTERM PRESENTATION

### MIDTERM REPORT

### ENDTERM PRESENTATION
- test older features from midterm
- weighting the samples



#### as Appexdix

### ENDTERM REPORT
- if Sac-x comparison doesnt work
  - we can still compare static/dynamic and with or without pretrining

- static vs dynamic : sac vs sac-u
- pretraining
  - bypass learning skills so that agent can deide the skills in a different way, should resemble to pretrained version
- combine sac-q + pretrain

### EXAM


## COLLAB TEAM 7 ?

